{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install and Load Packages**"
      ],
      "metadata": {
        "id": "MXZY3HasUgKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install tensorflow-addons\n",
        "! pip install -U tensorflow-addons"
      ],
      "metadata": {
        "id": "cqaK6gs7UsNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D, AveragePooling2D, concatenate\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, BatchNormalization, ReLU, Concatenate, AveragePooling2D\n",
        "# Import the unsupervised context normalization layer\n",
        "import os\n",
        "import sys\n",
        "package_dir = os.getcwd()\n",
        "root_dir = os.path.dirname(package_dir)\n",
        "sys.path.append(root_dir)\n",
        "from normalization.layers import UnsupervisedContextNormalization"
      ],
      "metadata": {
        "id": "SmtXMx_nUvMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions for standard normaization and save metrics and loss**"
      ],
      "metadata": {
        "id": "FXQTVFmcU3Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Mean and Standard Deviation\n",
        "def compute_mean_std(dataset):\n",
        "    data_r = np.dstack([dataset[i][:, :, 0] for i in range(len(dataset))])\n",
        "    data_g = np.dstack([dataset[i][:, :, 1] for i in range(len(dataset))])\n",
        "    data_b = np.dstack([dataset[i][:, :, 2] for i in range(len(dataset))])\n",
        "    mean = np.mean(data_r), np.mean(data_g), np.mean(data_b)\n",
        "    std = np.std(data_r), np.std(data_g), np.std(data_b)\n",
        "    return mean, std"
      ],
      "metadata": {
        "id": "9hpRtJhCVA4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save list to binary file\n",
        "def write_list(a_list, file_name):\n",
        "    # store list in binary file so 'wb' mode\n",
        "    with open(file_name, 'wb') as fp:\n",
        "        pickle.dump(a_list, fp)\n",
        "        print('Done writing list into a binary file')\n",
        "\n",
        "# Read list to memory\n",
        "def read_list(file_name):\n",
        "    # for reading also binary mode is important\n",
        "    with open(file_name, 'rb') as fp:\n",
        "        n_list = pickle.load(fp)\n",
        "        return n_list"
      ],
      "metadata": {
        "id": "EVlt9tOqVEPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define General DenseNet Architecture**"
      ],
      "metadata": {
        "id": "kPUXp2rrVKmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_block(x, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
        "    for _ in range(num_layers):\n",
        "        y = BatchNormalization()(x)\n",
        "        y = ReLU()(y)\n",
        "        y = Conv2D(bn_size * growth_rate, kernel_size=1, strides=1, use_bias=False)(y)\n",
        "        y = BatchNormalization()(y)\n",
        "        y = ReLU()(y)\n",
        "        y = Conv2D(growth_rate, kernel_size=3, strides=1, padding='same', use_bias=False)(y)\n",
        "        if drop_rate > 0:\n",
        "            y = tf.keras.layers.Dropout(drop_rate)(y)\n",
        "        x = Concatenate()([x, y])\n",
        "        num_input_features += growth_rate\n",
        "    return x, num_input_features\n",
        "\n",
        "def transition(x, num_input_features, num_output_features):\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(num_output_features, kernel_size=1, strides=1, use_bias=False)(x)\n",
        "    x = AveragePooling2D(pool_size=2, strides=2)(x)\n",
        "    return x, num_output_features // 2\n",
        "\n",
        "def DenseNet(num_layers, growth_rate=12, num_classes=100):\n",
        "    input_tensor = Input(shape=(32, 32, 3))\n",
        "    input_norm = UnsupervisedContextNormalization(num_components=10, epsilon=1e-3, momentum=0.9)(input_tensor, training=True)\n",
        "    x = Conv2D(2 * growth_rate, kernel_size=3, strides=1, padding='same', use_bias=False)(input_norm)\n",
        "    num_features = 2 * growth_rate\n",
        "\n",
        "    for i in range(3):\n",
        "        x, num_features = dense_block(x, num_layers, num_features, 4, growth_rate, 0.2)\n",
        "        if i < 2:\n",
        "            x, num_features = transition(x, num_features, num_features // 2)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=input_tensor, outputs=x)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "I4UKCaCpVPEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DenseNet-40**"
      ],
      "metadata": {
        "id": "_iUWADRiVXRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DenseNet with 40 Conv layers\n",
        "model_densenet40 = DenseNet(6)"
      ],
      "metadata": {
        "id": "psfG_MzqVaR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the models with Nesterov's accelerated gradient, weight decay, and momentum\n",
        "num_classes = 100\n",
        "batch_size = 64\n",
        "epochs = 200\n",
        "sgd = tf.keras.optimizers.SGD(0.1,momentum=0.9, nesterov=True,weight_decay=1e-4)\n",
        "model_densenet40.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=[\n",
        "            tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "            tf.keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "            tf.keras.metrics.Precision(name=\"precision\"),\n",
        "            tf.keras.metrics.Recall(name=\"recall\"),\n",
        "            tfa.metrics.F1Score(num_classes=num_classes, name=\"f1-score\")\n",
        "\n",
        "        ])"
      ],
      "metadata": {
        "id": "pw84pGdFVs6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a learning rate schedule\n",
        "def lr_schedule(epoch):\n",
        "    if epoch > 100 and epoch < 150:\n",
        "        return 0.01\n",
        "    elif epoch > 150:\n",
        "        return 0.001\n",
        "    return 0.1"
      ],
      "metadata": {
        "id": "Pj2OHmBsV66U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-100 dataset and perform data augmentation\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "W8KDu63AW6Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess CIFAR-100 dataset\n",
        "mean, std = compute_mean_std(x_train)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train = (x_train - mean) / std\n",
        "x_test = x_test.astype('float32')\n",
        "x_test = (x_test - mean) / std"
      ],
      "metadata": {
        "id": "7rpaFgPHXC7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Generator\n",
        "train_size = 40000\n",
        "x_val = x_train[train_size:]\n",
        "y_val = y_train[train_size:]\n",
        "\n",
        "x_train = x_train[:train_size]\n",
        "y_train = y_train[:train_size]\n",
        "\n",
        "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "datagen.fit(x_train)\n",
        "train_gen = datagen.flow(x=x_train, y=y_train, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "uo_sc8cpXEpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STEPS_PER_EPOCH = x_train.shape[0] // batch_size\n",
        "SAVE_PERIOD = 50"
      ],
      "metadata": {
        "id": "Fl8w_i66XKGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory to  save checkpoints\n",
        "checkpoint_dir = '.'\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_dir, 'model_weights_{epoch:03d}.h5'),\n",
        "    save_weights_only=True,\n",
        "    save_best_only=False,\n",
        "    save_freq=SAVE_PERIOD*STEPS_PER_EPOCH # Sauvegarder tous les 50 époques\n",
        ")\n",
        "\n",
        "# Train the DenseNet-40 model\n",
        "history = model_densenet40.fit(train_gen,\n",
        "                     steps_per_epoch=x_train.shape[0] // batch_size, epochs=epochs,\n",
        "                     validation_data=(x_val, y_val), callbacks=[keras.callbacks.LearningRateScheduler(lr_schedule), checkpoint_callback])"
      ],
      "metadata": {
        "id": "J5qcBzM2XO4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy, top_5_accuracy, precision, recall, f1 = model_densenet40.evaluate(x_test, y_test)\n",
        "print(f\"Test loss : {loss}\")\n",
        "print(f\"Test accuracy : {round(accuracy * 100, 2)}%\")\n",
        "print(f\"Test top 5 accuracy : {round(top_5_accuracy * 100, 2)}%\")\n",
        "print(f\"Precision : {round(precision * 100, 2)}%\")\n",
        "print(f\"Recall : {round(recall * 100, 2)}%\")\n",
        "print(f\"F1-score : {f1}%\")"
      ],
      "metadata": {
        "id": "8_PLI_BkX4Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the metrics and loss\n",
        "write_list(history.history['accuracy'], 'accuracy')\n",
        "write_list(history.history['val_accuracy'], 'val_accuracy')\n",
        "write_list(history.history['loss'], 'loss')\n",
        "write_list(history.history['val_loss'], 'val_loss')\n",
        "write_list(history.history['precision'], 'precision')\n",
        "write_list(history.history['val_precision'], 'val_precision')\n",
        "write_list(history.history['recall'], 'recall')\n",
        "write_list(history.history['val_recall'], 'val_recall')\n",
        "write_list(history.history['val_f1-score'], 'val_f1')\n",
        "write_list(history.history['f1-score'], 'f1')"
      ],
      "metadata": {
        "id": "rbgDaPZdYgqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DenseNet-100**"
      ],
      "metadata": {
        "id": "-UZy4na9ZAC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DenseNet-100\n",
        "model_densenet100 = DenseNet(16)"
      ],
      "metadata": {
        "id": "e9PZ6sopZEL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the models with Nesterov's accelerated gradient, weight decay, and momentum\n",
        "num_classes = 100\n",
        "batch_size = 64\n",
        "epochs = 200\n",
        "sgd = tf.keras.optimizers.SGD(0.1,momentum=0.9, nesterov=True,weight_decay=1e-4)\n",
        "model_densenet100.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=[\n",
        "            tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "            tf.keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "            tf.keras.metrics.Precision(name=\"precision\"),\n",
        "            tf.keras.metrics.Recall(name=\"recall\"),\n",
        "            tfa.metrics.F1Score(num_classes=num_classes, name=\"f1-score\")\n",
        "\n",
        "        ])"
      ],
      "metadata": {
        "id": "IWhc6bKqfUbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a learning rate schedule\n",
        "def lr_schedule(epoch):\n",
        "    if epoch > 100 and epoch < 150:\n",
        "        return 0.01\n",
        "    elif epoch >= 150:\n",
        "        return 0.001\n",
        "    return 0.1"
      ],
      "metadata": {
        "id": "sgkMX62hfuuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-100 dataset and perform data augmentation\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "pi9YOopwgfic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess CIFAR-100 dataset\n",
        "mean, std = compute_mean_std(x_train)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train = (x_train - mean) / std\n",
        "x_test = x_test.astype('float32')\n",
        "x_test = (x_test - mean) / std"
      ],
      "metadata": {
        "id": "go5O0D18ggRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Generator\n",
        "train_size = 40000\n",
        "x_val = x_train[train_size:]\n",
        "y_val = y_train[train_size:]\n",
        "\n",
        "x_train = x_train[:train_size]\n",
        "y_train = y_train[:train_size]\n",
        "\n",
        "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "datagen.fit(x_train)\n",
        "train_gen = datagen.flow(x=x_train, y=y_train, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "kRbEERxYgntJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    \"model.ckpt\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        ")\n",
        "# Train the DenseNet-100 model\n",
        "history = model_densenet100.fit(train_gen,\n",
        "                     steps_per_epoch=x_train.shape[0] // batch_size, epochs=epochs,\n",
        "                     validation_data=(x_val, y_val), callbacks=[keras.callbacks.LearningRateScheduler(lr_schedule), checkpoint_callback])\n"
      ],
      "metadata": {
        "id": "5MK75hjfgq1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model in the test set\n",
        "loss, accuracy, top_5_accuracy, precision, recall, f1 = model_densenet100.evaluate(x_test, y_test)\n",
        "print(f\"Test loss : {loss}\")\n",
        "print(f\"Test accuracy : {round(accuracy * 100, 2)}%\")\n",
        "print(f\"Test top 5 accuracy : {round(top_5_accuracy * 100, 2)}%\")\n",
        "print(f\"Precision : {round(precision * 100, 2)}%\")\n",
        "print(f\"Recall : {round(recall * 100, 2)}%\")\n",
        "print(f\"F1-score : {f1}%\")"
      ],
      "metadata": {
        "id": "xrLyj9pNgtAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save metrics and loss\n",
        "write_list(history.history['accuracy'], 'accuracy')\n",
        "write_list(history.history['val_accuracy'], 'val_accuracy')\n",
        "write_list(history.history['loss'], 'loss')\n",
        "write_list(history.history['val_loss'], 'val_loss')\n",
        "write_list(history.history['precision'], 'precision')\n",
        "write_list(history.history['val_precision'], 'val_precision')\n",
        "write_list(history.history['recall'], 'recall')\n",
        "write_list(history.history['val_recall'], 'val_recall')\n",
        "write_list(history.history['val_f1-score'], 'val_f1')\n",
        "write_list(history.history['f1-score'], 'f1')"
      ],
      "metadata": {
        "id": "6K-lSKmkgyR4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}