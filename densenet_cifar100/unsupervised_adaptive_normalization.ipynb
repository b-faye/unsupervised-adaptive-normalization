{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXZY3HasUgKz"
   },
   "source": [
    "# **Install and Load Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqaK6gs7UsNA"
   },
   "outputs": [],
   "source": [
    "# Install tensorflow-addons\n",
    "! pip install -U tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmtXMx_nUvMs"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D, AveragePooling2D, concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, BatchNormalization, ReLU, Concatenate, AveragePooling2D\n",
    "# Import the unsupervised adaptive normalization layer\n",
    "import os\n",
    "import sys\n",
    "package_dir = os.getcwd()\n",
    "root_dir = os.path.dirname(package_dir)\n",
    "sys.path.append(root_dir)\n",
    "from normalization.layers import UnsupervisedAdaptiveNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXQTVFmcU3Lz"
   },
   "source": [
    "# **Functions for standard normaization and save metrics and loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hpRtJhCVA4r"
   },
   "outputs": [],
   "source": [
    "# Compute Mean and Standard Deviation\n",
    "def compute_mean_std(dataset):\n",
    "    data_r = np.dstack([dataset[i][:, :, 0] for i in range(len(dataset))])\n",
    "    data_g = np.dstack([dataset[i][:, :, 1] for i in range(len(dataset))])\n",
    "    data_b = np.dstack([dataset[i][:, :, 2] for i in range(len(dataset))])\n",
    "    mean = np.mean(data_r), np.mean(data_g), np.mean(data_b)\n",
    "    std = np.std(data_r), np.std(data_g), np.std(data_b)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVlt9tOqVEPI"
   },
   "outputs": [],
   "source": [
    "# Save list to binary file\n",
    "def write_list(a_list, file_name):\n",
    "    # store list in binary file so 'wb' mode\n",
    "    with open(file_name, 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "        print('Done writing list into a binary file')\n",
    "\n",
    "# Read list to memory\n",
    "def read_list(file_name):\n",
    "    # for reading also binary mode is important\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPUXp2rrVKmT"
   },
   "source": [
    "# **Define General DenseNet Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4UKCaCpVPEy"
   },
   "outputs": [],
   "source": [
    "def dense_block(x, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "    for _ in range(num_layers):\n",
    "        y = BatchNormalization()(x)\n",
    "        y = ReLU()(y)\n",
    "        y = Conv2D(bn_size * growth_rate, kernel_size=1, strides=1, use_bias=False)(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = ReLU()(y)\n",
    "        y = Conv2D(growth_rate, kernel_size=3, strides=1, padding='same', use_bias=False)(y)\n",
    "        if drop_rate > 0:\n",
    "            y = tf.keras.layers.Dropout(drop_rate)(y)\n",
    "        x = Concatenate()([x, y])\n",
    "        num_input_features += growth_rate\n",
    "    return x, num_input_features\n",
    "\n",
    "def transition(x, num_input_features, num_output_features):\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(num_output_features, kernel_size=1, strides=1, use_bias=False)(x)\n",
    "    x = AveragePooling2D(pool_size=2, strides=2)(x)\n",
    "    return x, num_output_features // 2\n",
    "\n",
    "def DenseNet(num_layers, growth_rate=12, num_classes=100):\n",
    "    input_tensor = Input(shape=(32, 32, 3))\n",
    "    input_norm = UnsupervisedAdaptiveNormalization(num_components=10, epsilon=1e-3, momentum=0.9)(input_tensor, training=True)\n",
    "    x = Conv2D(2 * growth_rate, kernel_size=3, strides=1, padding='same', use_bias=False)(input_norm)\n",
    "    num_features = 2 * growth_rate\n",
    "\n",
    "    for i in range(3):\n",
    "        x, num_features = dense_block(x, num_layers, num_features, 4, growth_rate, 0.2)\n",
    "        if i < 2:\n",
    "            x, num_features = transition(x, num_features, num_features // 2)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_tensor, outputs=x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iUWADRiVXRY"
   },
   "source": [
    "# **DenseNet-40**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psfG_MzqVaR3"
   },
   "outputs": [],
   "source": [
    "# Create DenseNet with 40 Conv layers\n",
    "model_densenet40 = DenseNet(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw84pGdFVs6m"
   },
   "outputs": [],
   "source": [
    "# Compile the models with Nesterov's accelerated gradient, weight decay, and momentum\n",
    "num_classes = 100\n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "sgd = tf.keras.optimizers.SGD(0.1,momentum=0.9, nesterov=True,weight_decay=1e-4)\n",
    "model_densenet40.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=[\n",
    "            tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "            tf.keras.metrics.Precision(name=\"precision\"),\n",
    "            tf.keras.metrics.Recall(name=\"recall\"),\n",
    "            tfa.metrics.F1Score(num_classes=num_classes, name=\"f1-score\")\n",
    "\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pj2OHmBsV66U"
   },
   "outputs": [],
   "source": [
    "# Define a learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    if epoch > 100 and epoch < 150:\n",
    "        return 0.01\n",
    "    elif epoch > 150:\n",
    "        return 0.001\n",
    "    return 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8KDu63AW6Tn"
   },
   "outputs": [],
   "source": [
    "# Load the CIFAR-100 dataset and perform data augmentation\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rpaFgPHXC7A"
   },
   "outputs": [],
   "source": [
    "# Preprocess CIFAR-100 dataset\n",
    "mean, std = compute_mean_std(x_train)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = x_test.astype('float32')\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uo_sc8cpXEpo"
   },
   "outputs": [],
   "source": [
    "# Data Generator\n",
    "train_size = 40000\n",
    "x_val = x_train[train_size:]\n",
    "y_val = y_train[train_size:]\n",
    "\n",
    "x_train = x_train[:train_size]\n",
    "y_train = y_train[:train_size]\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "datagen.fit(x_train)\n",
    "train_gen = datagen.flow(x=x_train, y=y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fl8w_i66XKGP"
   },
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = x_train.shape[0] // batch_size\n",
    "SAVE_PERIOD = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5qcBzM2XO4R"
   },
   "outputs": [],
   "source": [
    "# Directory to  save checkpoints\n",
    "checkpoint_dir = '.'\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(checkpoint_dir, 'model_weights_{epoch:03d}.h5'),\n",
    "    save_weights_only=True,\n",
    "    save_best_only=False,\n",
    "    save_freq=SAVE_PERIOD*STEPS_PER_EPOCH # Sauvegarder tous les 50 Ã©poques\n",
    ")\n",
    "\n",
    "# Train the DenseNet-40 model\n",
    "history = model_densenet40.fit(train_gen,\n",
    "                     steps_per_epoch=x_train.shape[0] // batch_size, epochs=epochs,\n",
    "                     validation_data=(x_val, y_val), callbacks=[keras.callbacks.LearningRateScheduler(lr_schedule), checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_PLI_BkX4Xc"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy, top_5_accuracy, precision, recall, f1 = model_densenet40.evaluate(x_test, y_test)\n",
    "print(f\"Test loss : {loss}\")\n",
    "print(f\"Test accuracy : {round(accuracy * 100, 2)}%\")\n",
    "print(f\"Test top 5 accuracy : {round(top_5_accuracy * 100, 2)}%\")\n",
    "print(f\"Precision : {round(precision * 100, 2)}%\")\n",
    "print(f\"Recall : {round(recall * 100, 2)}%\")\n",
    "print(f\"F1-score : {f1}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbgDaPZdYgqG"
   },
   "outputs": [],
   "source": [
    "# Save the metrics and loss\n",
    "write_list(history.history['accuracy'], 'accuracy')\n",
    "write_list(history.history['val_accuracy'], 'val_accuracy')\n",
    "write_list(history.history['loss'], 'loss')\n",
    "write_list(history.history['val_loss'], 'val_loss')\n",
    "write_list(history.history['precision'], 'precision')\n",
    "write_list(history.history['val_precision'], 'val_precision')\n",
    "write_list(history.history['recall'], 'recall')\n",
    "write_list(history.history['val_recall'], 'val_recall')\n",
    "write_list(history.history['val_f1-score'], 'val_f1')\n",
    "write_list(history.history['f1-score'], 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UZy4na9ZAC1"
   },
   "source": [
    "# **DenseNet-100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9PZ6sopZEL4"
   },
   "outputs": [],
   "source": [
    "# Create DenseNet-100\n",
    "model_densenet100 = DenseNet(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWhc6bKqfUbg"
   },
   "outputs": [],
   "source": [
    "# Compile the models with Nesterov's accelerated gradient, weight decay, and momentum\n",
    "num_classes = 100\n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "sgd = tf.keras.optimizers.SGD(0.1,momentum=0.9, nesterov=True,weight_decay=1e-4)\n",
    "model_densenet100.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=[\n",
    "            tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "            tf.keras.metrics.Precision(name=\"precision\"),\n",
    "            tf.keras.metrics.Recall(name=\"recall\"),\n",
    "            tfa.metrics.F1Score(num_classes=num_classes, name=\"f1-score\")\n",
    "\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgkMX62hfuuU"
   },
   "outputs": [],
   "source": [
    "# Define a learning rate schedule\n",
    "def lr_schedule(epoch):\n",
    "    if epoch > 100 and epoch < 150:\n",
    "        return 0.01\n",
    "    elif epoch >= 150:\n",
    "        return 0.001\n",
    "    return 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pi9YOopwgfic"
   },
   "outputs": [],
   "source": [
    "# Load the CIFAR-100 dataset and perform data augmentation\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "go5O0D18ggRP"
   },
   "outputs": [],
   "source": [
    "# Preprocess CIFAR-100 dataset\n",
    "mean, std = compute_mean_std(x_train)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = x_test.astype('float32')\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRbEERxYgntJ"
   },
   "outputs": [],
   "source": [
    "# Data Generator\n",
    "train_size = 40000\n",
    "x_val = x_train[train_size:]\n",
    "y_val = y_train[train_size:]\n",
    "\n",
    "x_train = x_train[:train_size]\n",
    "y_train = y_train[:train_size]\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "datagen.fit(x_train)\n",
    "train_gen = datagen.flow(x=x_train, y=y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MK75hjfgq1u"
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    \"model.ckpt\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "# Train the DenseNet-100 model\n",
    "history = model_densenet100.fit(train_gen,\n",
    "                     steps_per_epoch=x_train.shape[0] // batch_size, epochs=epochs,\n",
    "                     validation_data=(x_val, y_val), callbacks=[keras.callbacks.LearningRateScheduler(lr_schedule), checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrLyj9pNgtAX"
   },
   "outputs": [],
   "source": [
    "# Evaluate model in the test set\n",
    "loss, accuracy, top_5_accuracy, precision, recall, f1 = model_densenet100.evaluate(x_test, y_test)\n",
    "print(f\"Test loss : {loss}\")\n",
    "print(f\"Test accuracy : {round(accuracy * 100, 2)}%\")\n",
    "print(f\"Test top 5 accuracy : {round(top_5_accuracy * 100, 2)}%\")\n",
    "print(f\"Precision : {round(precision * 100, 2)}%\")\n",
    "print(f\"Recall : {round(recall * 100, 2)}%\")\n",
    "print(f\"F1-score : {f1}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6K-lSKmkgyR4"
   },
   "outputs": [],
   "source": [
    "# Save metrics and loss\n",
    "write_list(history.history['accuracy'], 'accuracy')\n",
    "write_list(history.history['val_accuracy'], 'val_accuracy')\n",
    "write_list(history.history['loss'], 'loss')\n",
    "write_list(history.history['val_loss'], 'val_loss')\n",
    "write_list(history.history['precision'], 'precision')\n",
    "write_list(history.history['val_precision'], 'val_precision')\n",
    "write_list(history.history['recall'], 'recall')\n",
    "write_list(history.history['val_recall'], 'val_recall')\n",
    "write_list(history.history['val_f1-score'], 'val_f1')\n",
    "write_list(history.history['f1-score'], 'f1')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
