{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ANCMaZtuICz"
   },
   "source": [
    "# **Install and Load Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cojeGK26uDY9"
   },
   "outputs": [],
   "source": [
    "# Install tensorflow-addons\n",
    "! pip install -U tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xW7negeuVaL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.datasets import cifar100, cifar10\n",
    "import tensorflow_datasets as tfds\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy.random import rand\n",
    "from pylab import figure\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.utils import shuffle\n",
    "import cv2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.ndimage as nd\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, SparseTopKCategoricalAccuracy, Precision, Recall\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# import keras_cv\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, SparseTopKCategoricalAccuracy, Precision, Recall\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# Import the unsupervised adaptive normalization layer\n",
    "import os\n",
    "import sys\n",
    "package_dir = os.getcwd()\n",
    "root_dir = os.path.dirname(package_dir)\n",
    "sys.path.append(root_dir)\n",
    "from normalization.layers import UnsupervisedAdaptiveNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "murI05Uyujpx"
   },
   "source": [
    "# **Functions and Configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUpvO-mnum_Z"
   },
   "outputs": [],
   "source": [
    "# Compute Mean and Standard Deviation\n",
    "def compute_mean_std(dataset):\n",
    "    data_r = np.dstack([dataset[i][:, :, 0] for i in range(len(dataset))])\n",
    "    data_g = np.dstack([dataset[i][:, :, 1] for i in range(len(dataset))])\n",
    "    data_b = np.dstack([dataset[i][:, :, 2] for i in range(len(dataset))])\n",
    "    mean = np.mean(data_r), np.mean(data_g), np.mean(data_b)\n",
    "    std = np.std(data_r), np.std(data_g), np.std(data_b)\n",
    "    return mean, std\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Resizing(72, 72),\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(factor=0.02),\n",
    "        tf.keras.layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "\n",
    "# Save list to binary file\n",
    "def write_list(a_list, file_name):\n",
    "    # store list in binary file so 'wb' mode\n",
    "    with open(file_name, 'wb') as fp:\n",
    "        pickle.dump(a_list, fp)\n",
    "        print('Done writing list into a binary file')\n",
    "\n",
    "# Read list to memory\n",
    "def read_list(file_name):\n",
    "    # for reading also binary mode is important\n",
    "    with open(file_name, 'rb') as fp:\n",
    "        n_list = pickle.load(fp)\n",
    "        return n_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9iFZ46LvjJu"
   },
   "source": [
    "# **CIFAR-10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NvaGhi6Eu04s"
   },
   "outputs": [],
   "source": [
    "# Define constant parameters\n",
    "class CFG:\n",
    "    batch_size = 256\n",
    "    learning_rate = 0.005\n",
    "    weight_decay = 1e-4\n",
    "    num_classes = 10\n",
    "    num_clusters = 3 # Number of clusters to be estimated\n",
    "    num_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGjJEORyv3y1"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Encod labels with one-hot representation\n",
    "y_train_sparse = to_categorical(y_train, num_classes=CFG.num_classes)\n",
    "y_test_sparse = to_categorical(y_test, num_classes=CFG.num_classes)\n",
    "\n",
    "# Standardize the dataset using the defined function computer_mean_std\n",
    "mean, std = compute_mean_std(x_train)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = x_test.astype('float32')\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTRJiWkuwGXq"
   },
   "outputs": [],
   "source": [
    "# Build CNN\n",
    "def build_cnn(num_classes=CFG.num_classes, num_clusters=CFG.num_clusters, learning_rate=CFG.learning_rate, weight_decay=CFG.weight_decay):\n",
    "    input_image = tf.keras.layers.Input(shape=(32,32,3))\n",
    "    augmented = data_augmentation(input_image)\n",
    "\n",
    "    conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(augmented)\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = tf.keras.layers.ReLU()(conv1)\n",
    "\n",
    "    pool1 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv2D(filters=128, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(pool1)\n",
    "    conv2 = UnsupervisedAdaptiveNormalization(num_components=num_clusters, epsilon=1e-3, momentum=0.9)(conv2, training=True)\n",
    "    conv2 = tf.keras.layers.ReLU()(conv2)\n",
    "\n",
    "    pool2 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(pool2)\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = tf.keras.layers.ReLU()(conv3)\n",
    "\n",
    "    pool3 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2, 2), padding=\"same\")(conv3)\n",
    "\n",
    "    conv4 = tf.keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1),  padding=\"same\")(pool3)\n",
    "    conv4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = tf.keras.layers.ReLU()(conv4)\n",
    "\n",
    "    pool4 = tf.keras.layers.AveragePooling2D(pool_size=(4, 4), strides=(1, 1))(conv4)\n",
    "    flattened = tf.keras.layers.Flatten()(pool4)\n",
    "    outputs = tf.keras.layers.Dense(num_classes)(flattened)\n",
    "\n",
    "    model = tf.keras.models.Model(input_image, outputs)\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "            Precision(name=\"precision\"),\n",
    "            Recall(name=\"recall\"),\n",
    "            tfa.metrics.F1Score(num_classes=num_classes, name=\"f1-score\")\n",
    "\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFazYnqnxgF9"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def run_model(model, filepath, batch_size=CFG.batch_size, num_epochs=CFG.num_epochs):\n",
    "    checkpoint_filepath = filepath\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train_sparse,\n",
    "        batch_size=batch_size,\n",
    "        epochs=CFG.num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  model = build_cnn()\n",
    "  history = run_model(model, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYzImQNxxusY"
   },
   "outputs": [],
   "source": [
    "# save metrics and loss\n",
    "write_list(history.history['accuracy'], 'accuracy')\n",
    "write_list(history.history['val_accuracy'], 'val_accuracy')\n",
    "write_list(history.history['loss'], 'loss')\n",
    "write_list(history.history['val_loss'], 'val_loss')\n",
    "write_list(history.history['precision'], 'precision')\n",
    "write_list(history.history['val_precision'], 'val_precision')\n",
    "write_list(history.history['recall'], 'recall')\n",
    "write_list(history.history['val_recall'], 'val_recall')\n",
    "write_list(history.history['val_f1-score'], 'val_f1')\n",
    "write_list(history.history['f1-score'], 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQARdM_pyLrw"
   },
   "source": [
    "# **CIFAR-100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eTljdFPyLrx"
   },
   "outputs": [],
   "source": [
    "# Define constant parameters\n",
    "class CFG:\n",
    "    batch_size = 256\n",
    "    learning_rate = 0.005\n",
    "    weight_decay = 1e-4\n",
    "    num_classes = 100\n",
    "    num_clusters = 3\n",
    "    num_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8j980wGyLry"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR-100 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "# Encod labels with one-hot representation\n",
    "y_train_sparse = to_categorical(y_train, num_classes=CFG.num_classes)\n",
    "y_test_sparse = to_categorical(y_test, num_classes=CFG.num_classes)\n",
    "\n",
    "# Standardize the dataset using the defined function computer_mean_std\n",
    "mean, std = compute_mean_std(x_train)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = x_test.astype('float32')\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNr-JPFqyLry"
   },
   "outputs": [],
   "source": [
    "# Build CNN\n",
    "def build_cnn(num_classes=CFG.num_classes, num_clusters=CFG.num_clusters, learning_rate=CFG.learning_rate, weight_decay=CFG.weight_decay):\n",
    "    input_image = tf.keras.layers.Input(shape=(32,32,3))\n",
    "    augmented = data_augmentation(input_image)\n",
    "\n",
    "    conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(augmented)\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = tf.keras.layers.ReLU()(conv1)\n",
    "\n",
    "    pool1 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv2D(filters=128, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(pool1)\n",
    "    conv2 = UnsupervisedAdaptiveNormalization(num_components=num_clusters, epsilon=1e-3, momentum=0.9)(conv2, training=True)\n",
    "    conv2 = tf.keras.layers.ReLU()(conv2)\n",
    "\n",
    "    pool2 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(pool2)\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = tf.keras.layers.ReLU()(conv3)\n",
    "\n",
    "    pool3 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2, 2), padding=\"same\")(conv3)\n",
    "\n",
    "    conv4 = tf.keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1),  padding=\"same\")(pool3)\n",
    "    conv4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = tf.keras.layers.ReLU()(conv4)\n",
    "\n",
    "    pool4 = tf.keras.layers.AveragePooling2D(pool_size=(4, 4), strides=(1, 1))(conv4)\n",
    "    flattened = tf.keras.layers.Flatten()(pool4)\n",
    "    outputs = tf.keras.layers.Dense(num_classes)(flattened)\n",
    "\n",
    "    model = tf.keras.models.Model(input_image, outputs)\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "            Precision(name=\"precision\"),\n",
    "            Recall(name=\"recall\"),\n",
    "            tfa.metrics.F1Score(num_classes=num_classes, name=\"f1-score\")\n",
    "\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lhzy5oP1yLr0"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def run_model(model, filepath, batch_size=CFG.batch_size, num_epochs=CFG.num_epochs):\n",
    "    checkpoint_filepath = filepath\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train_sparse,\n",
    "        batch_size=batch_size,\n",
    "        epochs=CFG.num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  model = build_cnn()\n",
    "  history = run_model(model, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAiDA8m3yLr1"
   },
   "outputs": [],
   "source": [
    "# save metrics and loss\n",
    "write_list(history.history['accuracy'], 'accuracy')\n",
    "write_list(history.history['val_accuracy'], 'val_accuracy')\n",
    "write_list(history.history['loss'], 'loss')\n",
    "write_list(history.history['val_loss'], 'val_loss')\n",
    "write_list(history.history['precision'], 'precision')\n",
    "write_list(history.history['val_precision'], 'val_precision')\n",
    "write_list(history.history['recall'], 'recall')\n",
    "write_list(history.history['val_recall'], 'val_recall')\n",
    "write_list(history.history['val_f1-score'], 'val_f1')\n",
    "write_list(history.history['f1-score'], 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYseJP4JyW3Q"
   },
   "source": [
    "# **Tiny ImageNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBFsTryLyaTg"
   },
   "outputs": [],
   "source": [
    "# Clone ImageNet dataset\n",
    "! git clone https://github.com/seshuad/IMagenet\n",
    "! ls 'IMagenet/tiny-imagenet-200/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6lzTIAJyfns"
   },
   "outputs": [],
   "source": [
    "# Define all parameters\n",
    "class CFG:\n",
    "    projection_dims = 256\n",
    "    batch_size = 256\n",
    "    epochs = 100\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 1e-4\n",
    "    num_classes = 200\n",
    "    num_clusters = 3\n",
    "    num_epochs = 100\n",
    "    image_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHn4SUPMyi4Y"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load dataset and split into train and test\n",
    "path = 'IMagenet/tiny-imagenet-200/'\n",
    "\n",
    "def get_id_dictionary():\n",
    "    id_dict = {}\n",
    "    for i, line in enumerate(open( path + 'wnids.txt', 'r')):\n",
    "        id_dict[line.replace('\\n', '')] = i\n",
    "    return id_dict\n",
    "\n",
    "def get_class_to_id_dict():\n",
    "    id_dict = get_id_dictionary()\n",
    "    all_classes = {}\n",
    "    result = {}\n",
    "    for i, line in enumerate(open( path + 'words.txt', 'r')):\n",
    "        n_id, word = line.split('\\t')[:2]\n",
    "        all_classes[n_id] = word\n",
    "    for key, value in id_dict.items():\n",
    "        result[value] = (key, all_classes[key])\n",
    "    return result\n",
    "\n",
    "def get_data(id_dict):\n",
    "    print('starting loading data')\n",
    "    train_data, test_data = [], []\n",
    "    train_labels, test_labels = [], []\n",
    "    t = time.time()\n",
    "\n",
    "    for key, value in id_dict.items():\n",
    "        train_data += [nd.imread( path + 'train/{}/images/{}_{}.JPEG'.format(key, key, str(i)), pilmode='RGB') for i in range(500)]\n",
    "        train_labels_ = np.array([[0]*200]*500)\n",
    "        train_labels_[:, value] = 1\n",
    "        train_labels += train_labels_.tolist()\n",
    "\n",
    "    for line in open( path + 'val/val_annotations.txt'):\n",
    "        img_name, class_id = line.split('\\t')[:2]\n",
    "        test_data.append(nd.imread( path + 'val/images/{}'.format(img_name) ,pilmode='RGB'))\n",
    "        test_labels_ = np.array([[0]*200])\n",
    "        test_labels_[0, id_dict[class_id]] = 1\n",
    "        test_labels += test_labels_.tolist()\n",
    "\n",
    "    print('finished loading data, in {} seconds'.format(time.time() - t))\n",
    "    return np.array(train_data), np.array(train_labels), np.array(test_data), np.array(test_labels)\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = get_data(get_id_dictionary())\n",
    "print( \"train data shape: \",  train_data.shape )\n",
    "print( \"train label shape: \", train_labels.shape )\n",
    "print( \"test data shape: \",   test_data.shape )\n",
    "print( \"test_labels.shape: \", test_labels.shape )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h55M246Dyn3e"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \"\"\" Helper to iterate over the data (as Numpy arrays). \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img, target_img, mean, std, number_classes):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img = input_img\n",
    "        self.target_img = target_img\n",
    "        self.mean = mean\n",
    "        self.number_classes = number_classes\n",
    "        self.std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Returns tuple (input, target) correspond to batch #idx. \"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img = self.input_img[i : i + self.batch_size]\n",
    "        batch_target_img = self.target_img[i : i + self.batch_size]\n",
    "\n",
    "        # images\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, image in enumerate(batch_input_img):\n",
    "          x[j] = image\n",
    "        x = x/255.0\n",
    "        x = (x - self.mean)/self.std\n",
    "\n",
    "        # labels\n",
    "        y = np.zeros((self.batch_size, self.number_classes))\n",
    "        for j, target in enumerate(batch_target_img):\n",
    "          y[j] = target\n",
    "\n",
    "        return (x), y\n",
    "\n",
    "idx = np.random.permutation(len(train_data))\n",
    "x_train, y_train = train_data[idx], train_labels[idx]\n",
    "idx = np.random.permutation(len(test_data))\n",
    "x_test, y_test = test_data[idx], test_labels[idx]\n",
    "mean=(0.485, 0.456, 0.406)\n",
    "std=(0.229, 0.224, 0.225)\n",
    "train_data, train_labels, test_data, test_labels\n",
    "x_val = x_train[92000:]\n",
    "y_val = y_train[92000:]\n",
    "train_generator = DataGenerator(CFG.batch_size, (CFG.image_size, CFG.image_size), x_train[:92000], y_train[:92000], mean, std, CFG.num_classes)\n",
    "test_generator = DataGenerator(CFG.batch_size, (CFG.image_size, CFG.image_size), x_test, y_test, mean, std, CFG.num_classes)\n",
    "val_generator = DataGenerator(CFG.batch_size, (CFG.image_size, CFG.image_size), x_val, y_val, mean, std, CFG.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INJt54MP0hxn"
   },
   "outputs": [],
   "source": [
    "# Build CNN\n",
    "def build_cnn(num_classes=CFG.num_classes, num_clusters=CFG.num_clusters, learning_rate=CFG.learning_rate, weight_decay=CFG.weight_decay):\n",
    "    input_image = tf.keras.layers.Input(shape=(32,32,3))\n",
    "    augmented = data_augmentation(input_image)\n",
    "\n",
    "    conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(augmented)\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = tf.keras.layers.ReLU()(conv1)\n",
    "\n",
    "    pool1 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv2D(filters=128, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(pool1)\n",
    "    conv2 = UnsupervisedAdaptiveNormalization(num_components=num_clusters, epsilon=1e-3, momentum=0.9)(conv2, training=True)\n",
    "    conv2 = tf.keras.layers.ReLU()(conv2)\n",
    "\n",
    "    pool2 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv2D(filters=128, kernel_size=(5,5), strides=(1,1),  padding=\"same\")(pool2)\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = tf.keras.layers.ReLU()(conv3)\n",
    "\n",
    "    pool3 = tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2, 2), padding=\"same\")(conv3)\n",
    "\n",
    "    conv4 = tf.keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1),  padding=\"same\")(pool3)\n",
    "    conv4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = tf.keras.layers.ReLU()(conv4)\n",
    "\n",
    "    pool4 = tf.keras.layers.AveragePooling2D(pool_size=(4, 4), strides=(1, 1))(conv4)\n",
    "    flattened = tf.keras.layers.Flatten()(pool4)\n",
    "    outputs = tf.keras.layers.Dense(num_classes)(flattened)\n",
    "\n",
    "    model = tf.keras.models.Model(input_image, outputs)\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "            Precision(name=\"precision\"),\n",
    "            Recall(name=\"recall\"),\n",
    "            tfa.metrics.F1Score(num_classes=num_classes, name=\"f1-score\")\n",
    "\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8fFqKqg0nGA"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def run_model(model, filepath, batch_size=CFG.batch_size, num_epochs=CFG.num_epochs):\n",
    "    checkpoint_filepath = filepath\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        batch_size=batch_size,\n",
    "        validation_data = val_generator,\n",
    "        epochs=num_epochs,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  model = build_cnn()\n",
    "  history = run_model(model, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEAccDY_1AOu"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "write_list(history.history['accuracy'], 'accuracy')\n",
    "write_list(history.history['val_accuracy'], 'val_accuracy')\n",
    "write_list(history.history['loss'], 'loss')\n",
    "write_list(history.history['val_loss'], 'val_loss')\n",
    "write_list(history.history['precision'], 'precision')\n",
    "write_list(history.history['val_precision'], 'val_precision')\n",
    "write_list(history.history['recall'], 'recall')\n",
    "write_list(history.history['val_recall'], 'val_recall')\n",
    "write_list(history.history['val_f1-score'], 'val_f1')\n",
    "write_list(history.history['f1-score'], 'f1')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
